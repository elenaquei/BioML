{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.loader import DataLoader\n",
    "from data_creation import create_dataset, to_pyg_data_true\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import train_test_split_edges\n",
    "from torch_geometric.nn import GAE\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch_geometric.utils import negative_sampling\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os.path as osp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])\n",
      "Data(x=[2708, 1433], val_pos_edge_index=[2, 263], test_pos_edge_index=[2, 527], train_pos_edge_index=[2, 8976], train_neg_adj_mask=[2708, 2708], val_neg_edge_index=[2, 263], test_neg_edge_index=[2, 527])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lucas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch_geometric\\data\\in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "c:\\Users\\Lucas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'train_test_split_edges' is deprecated, use 'transforms.RandomLinkSplit' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "# load the Cora dataset\n",
    "dataset = 'Cora'\n",
    "path = osp.join('.', 'data', dataset)\n",
    "dataset = Planetoid(path, dataset, transform=T.NormalizeFeatures())\n",
    "data = dataset[0]\n",
    "print(dataset.data)\n",
    "\n",
    "# use train_test_split_edges to create neg and positive edges\n",
    "data.train_mask = data.val_mask = data.test_mask = data.y = None\n",
    "data = train_test_split_edges(data)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 10000])\n",
      "Data(x=[100, 10000], y=[10000], val_pos_edge_index=[2, 144], val_pos_edge_attr=[144], test_pos_edge_index=[2, 144], test_pos_edge_attr=[144], train_pos_edge_index=[2, 2316], train_pos_edge_attr=[2316], train_neg_adj_mask=[100, 100], val_neg_edge_index=[2, 144], test_neg_edge_index=[2, 144])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lucas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'train_test_split_edges' is deprecated, use 'transforms.RandomLinkSplit' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "# Generate just one huge graph\n",
    "ode_dim = 100\n",
    "n_data = 100\n",
    "tp = torch.tensor(np.linspace(0,1,100))\n",
    "x_exact, x_train, y_train, param = create_dataset(ode_dim,n_data,1,tp)\n",
    "num_features = n_data*len(tp)\n",
    "\n",
    "dataset_train = []\n",
    "for i in range(0,len(x_train)):\n",
    "    p = param[i]\n",
    "    param_tensor = torch.cat((p.Win.flatten(),p.Wout.flatten(),p.bin.flatten(),p.bout.flatten(),p.gamma.flatten()))\n",
    "    dataset_train.append(to_pyg_data_true(x_train[i],y_train[i],ode_dim,n_data*len(tp)))\n",
    "\n",
    "data = dataset_train[0]\n",
    "\n",
    "print(data.x.shape)\n",
    "\n",
    "data = train_test_split_edges(data, val_ratio=0.1, test_ratio=0.1)\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self,input_dim,hidden_dim=200):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.conv3 = GCNConv(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.lin1 = torch.nn.Linear(hidden_dim, hidden_dim).weight\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(p=0.7)\n",
    "\n",
    "    def encode(self):\n",
    "        x = self.conv1(data.x, data.train_pos_edge_index) # convolution 1\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv2(x, data.train_pos_edge_index) # convolution 2\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.dropout(x)\n",
    "        return self.conv3(x, data.train_pos_edge_index) # convolution 3\n",
    "\n",
    "    def decode(self, z, pos_edge_index, neg_edge_index): # only pos and neg edges\n",
    "        edge_index = torch.cat([pos_edge_index, neg_edge_index], dim=-1) # concatenate pos and neg edges\n",
    "        logits = (z[edge_index[0]] * torch.matmul(self.lin1,z[edge_index[1]].t()).t()).sum(dim=-1)  # dot product with learnable parameter\n",
    "        return logits\n",
    "\n",
    "    def decode_all(self, z): \n",
    "        prob_adj = z @ z.t() # get adj NxN\n",
    "        return (prob_adj > 0).nonzero(as_tuple=False).t() # get predicted edge_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model, data = Net(num_features).to(device), data.to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_link_labels(pos_edge_index, neg_edge_index):\n",
    "    # returns a tensor:\n",
    "    # [1,1,1,1,...,0,0,0,0,0,..] with the number of ones is equel to the length of pos_edge_index\n",
    "    # and the number of zeros is equal to the length of neg_edge_index\n",
    "    E = pos_edge_index.size(1) + neg_edge_index.size(1)\n",
    "    link_labels = torch.zeros(E, dtype=torch.float, device=device)\n",
    "    link_labels[:pos_edge_index.size(1)] = 1.\n",
    "    return link_labels\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    neg_edge_index = negative_sampling(\n",
    "        edge_index=data.train_pos_edge_index, #positive edges\n",
    "        num_nodes=data.num_nodes, # number of nodes\n",
    "        num_neg_samples=data.train_pos_edge_index.size(1)) # number of neg_sample equal to number of pos_edges\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    z = model.encode() #encode\n",
    "    link_logits = model.decode(z, data.train_pos_edge_index, neg_edge_index) # decode\n",
    "    \n",
    "    link_labels = get_link_labels(data.train_pos_edge_index, neg_edge_index)\n",
    "    loss = F.binary_cross_entropy_with_logits(link_logits, link_labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    perfs = []\n",
    "    for prefix in [\"val\", \"test\"]:\n",
    "        pos_edge_index = data[f'{prefix}_pos_edge_index']\n",
    "        neg_edge_index = data[f'{prefix}_neg_edge_index']\n",
    "\n",
    "        z = model.encode() # encode train\n",
    "        link_logits = model.decode(z, pos_edge_index, neg_edge_index) # decode test or val\n",
    "        link_probs = link_logits.sigmoid() # apply sigmoid\n",
    "\n",
    "        link_labels = get_link_labels(pos_edge_index, neg_edge_index) # get link\n",
    "        #print(link_probs[:len(pos_edge_index)].mean())\n",
    "        #print(link_probs[len(pos_edge_index):].mean())\n",
    "        perfs.append(roc_auc_score(link_labels.cpu(), link_probs.cpu())) #compute roc_auc score\n",
    "    return perfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 010, Loss: 79064.8984, Val: 0.5000, Test: 0.5000\n",
      "Epoch: 020, Loss: 43703.2188, Val: 0.5000, Test: 0.5000\n",
      "Epoch: 030, Loss: 1854.6610, Val: 0.5000, Test: 0.5000\n",
      "Epoch: 040, Loss: 2006.6151, Val: 0.5000, Test: 0.5000\n",
      "Epoch: 050, Loss: 1359.0347, Val: 0.5000, Test: 0.5000\n",
      "Epoch: 060, Loss: 987.5823, Val: 0.5000, Test: 0.5000\n",
      "Epoch: 070, Loss: 347.6730, Val: 0.5018, Test: 0.5001\n",
      "Epoch: 080, Loss: 386.6512, Val: 0.5050, Test: 0.5080\n",
      "Epoch: 090, Loss: 382.1568, Val: 0.5050, Test: 0.5080\n",
      "Epoch: 100, Loss: 261.9381, Val: 0.5050, Test: 0.5080\n",
      "Epoch: 110, Loss: 321.5575, Val: 0.5050, Test: 0.5080\n",
      "Epoch: 120, Loss: 138.0269, Val: 0.5050, Test: 0.5080\n",
      "Epoch: 130, Loss: 96.8917, Val: 0.5050, Test: 0.5080\n",
      "Epoch: 140, Loss: 299.4524, Val: 0.5050, Test: 0.5080\n",
      "Epoch: 150, Loss: 190.9396, Val: 0.5050, Test: 0.5080\n",
      "Epoch: 160, Loss: 101.0290, Val: 0.5050, Test: 0.5080\n",
      "Epoch: 170, Loss: 83.4568, Val: 0.5050, Test: 0.5080\n",
      "Epoch: 180, Loss: 101.6479, Val: 0.5050, Test: 0.5080\n",
      "Epoch: 190, Loss: 100.8453, Val: 0.5052, Test: 0.4961\n",
      "Epoch: 200, Loss: 217.6547, Val: 0.5052, Test: 0.4961\n",
      "Epoch: 210, Loss: 165.6222, Val: 0.5052, Test: 0.4961\n",
      "Epoch: 220, Loss: 78.8437, Val: 0.5052, Test: 0.4961\n",
      "Epoch: 230, Loss: 62.0566, Val: 0.5052, Test: 0.4961\n",
      "Epoch: 240, Loss: 61.0136, Val: 0.5067, Test: 0.5207\n",
      "Epoch: 250, Loss: 47.1308, Val: 0.5067, Test: 0.5207\n",
      "Epoch: 260, Loss: 72.9954, Val: 0.5067, Test: 0.5207\n",
      "Epoch: 270, Loss: 43.9403, Val: 0.5067, Test: 0.5207\n",
      "Epoch: 280, Loss: 71.1981, Val: 0.5067, Test: 0.5207\n",
      "Epoch: 290, Loss: 70.7053, Val: 0.5067, Test: 0.5207\n",
      "Epoch: 300, Loss: 69.8862, Val: 0.5067, Test: 0.5207\n",
      "Epoch: 310, Loss: 82.5685, Val: 0.5067, Test: 0.5207\n",
      "Epoch: 320, Loss: 78.3473, Val: 0.5067, Test: 0.5207\n",
      "Epoch: 330, Loss: 79.1032, Val: 0.5067, Test: 0.5207\n",
      "Epoch: 340, Loss: 66.9334, Val: 0.5099, Test: 0.4786\n",
      "Epoch: 350, Loss: 72.0590, Val: 0.5099, Test: 0.4786\n",
      "Epoch: 360, Loss: 42.2664, Val: 0.5099, Test: 0.4786\n",
      "Epoch: 370, Loss: 46.5289, Val: 0.5099, Test: 0.4786\n",
      "Epoch: 380, Loss: 49.7173, Val: 0.5099, Test: 0.4786\n",
      "Epoch: 390, Loss: 48.9190, Val: 0.5099, Test: 0.4786\n",
      "Epoch: 400, Loss: 39.8309, Val: 0.5099, Test: 0.4786\n",
      "Epoch: 410, Loss: 54.9966, Val: 0.5099, Test: 0.4786\n",
      "Epoch: 420, Loss: 37.2264, Val: 0.5099, Test: 0.4786\n",
      "Epoch: 430, Loss: 58.9632, Val: 0.5099, Test: 0.4786\n",
      "Epoch: 440, Loss: 46.2230, Val: 0.5099, Test: 0.4786\n",
      "Epoch: 450, Loss: 44.0316, Val: 0.5099, Test: 0.4786\n",
      "Epoch: 460, Loss: 25.8727, Val: 0.5099, Test: 0.4786\n",
      "Epoch: 470, Loss: 37.7399, Val: 0.5103, Test: 0.5207\n",
      "Epoch: 480, Loss: 35.9012, Val: 0.5103, Test: 0.5207\n",
      "Epoch: 490, Loss: 37.1074, Val: 0.5103, Test: 0.5207\n",
      "Epoch: 500, Loss: 31.6463, Val: 0.5103, Test: 0.5207\n",
      "Epoch: 510, Loss: 24.3161, Val: 0.5103, Test: 0.5207\n",
      "Epoch: 520, Loss: 95.6845, Val: 0.5103, Test: 0.5207\n",
      "Epoch: 530, Loss: 45.4255, Val: 0.5103, Test: 0.5207\n",
      "Epoch: 540, Loss: 58.6305, Val: 0.5103, Test: 0.5207\n",
      "Epoch: 550, Loss: 127.8462, Val: 0.5103, Test: 0.5207\n",
      "Epoch: 560, Loss: 32.1301, Val: 0.5103, Test: 0.5207\n",
      "Epoch: 570, Loss: 26.9731, Val: 0.5103, Test: 0.5207\n",
      "Epoch: 580, Loss: 39.4960, Val: 0.5103, Test: 0.5207\n",
      "Epoch: 590, Loss: 32.0501, Val: 0.5103, Test: 0.5207\n",
      "Epoch: 600, Loss: 30.6364, Val: 0.5103, Test: 0.5207\n",
      "Epoch: 610, Loss: 51.5130, Val: 0.5177, Test: 0.5055\n",
      "Epoch: 620, Loss: 30.2635, Val: 0.5177, Test: 0.5055\n",
      "Epoch: 630, Loss: 37.3910, Val: 0.5177, Test: 0.5055\n",
      "Epoch: 640, Loss: 35.8444, Val: 0.5177, Test: 0.5055\n",
      "Epoch: 650, Loss: 23.5459, Val: 0.5177, Test: 0.5055\n",
      "Epoch: 660, Loss: 20.7289, Val: 0.5177, Test: 0.5055\n",
      "Epoch: 670, Loss: 35.9401, Val: 0.5177, Test: 0.5055\n",
      "Epoch: 680, Loss: 28.7346, Val: 0.5177, Test: 0.5055\n",
      "Epoch: 690, Loss: 25.5093, Val: 0.5177, Test: 0.5055\n",
      "Epoch: 700, Loss: 21.2219, Val: 0.5177, Test: 0.5055\n",
      "Epoch: 710, Loss: 29.4089, Val: 0.5177, Test: 0.5055\n",
      "Epoch: 720, Loss: 19.7374, Val: 0.5177, Test: 0.5055\n",
      "Epoch: 730, Loss: 24.7347, Val: 0.5177, Test: 0.5055\n",
      "Epoch: 740, Loss: 28.7849, Val: 0.5177, Test: 0.5055\n",
      "Epoch: 750, Loss: 19.9422, Val: 0.5177, Test: 0.5055\n",
      "Epoch: 760, Loss: 37.0067, Val: 0.5177, Test: 0.5055\n",
      "Epoch: 770, Loss: 32.4328, Val: 0.5177, Test: 0.5055\n",
      "Epoch: 780, Loss: 24.6629, Val: 0.5177, Test: 0.5055\n",
      "Epoch: 790, Loss: 19.8996, Val: 0.5177, Test: 0.5055\n",
      "Epoch: 800, Loss: 32.9920, Val: 0.5177, Test: 0.5055\n",
      "Epoch: 810, Loss: 51.7331, Val: 0.5177, Test: 0.5055\n",
      "Epoch: 820, Loss: 18.7178, Val: 0.5177, Test: 0.5055\n",
      "Epoch: 830, Loss: 35.6270, Val: 0.5177, Test: 0.5055\n",
      "Epoch: 840, Loss: 21.8408, Val: 0.5177, Test: 0.5055\n",
      "Epoch: 850, Loss: 16.1412, Val: 0.5177, Test: 0.5055\n",
      "Epoch: 860, Loss: 20.4285, Val: 0.5351, Test: 0.4991\n",
      "Epoch: 870, Loss: 20.2430, Val: 0.5351, Test: 0.4991\n",
      "Epoch: 880, Loss: 32.2566, Val: 0.5351, Test: 0.4991\n",
      "Epoch: 890, Loss: 27.3217, Val: 0.5351, Test: 0.4991\n",
      "Epoch: 900, Loss: 15.6034, Val: 0.5351, Test: 0.4991\n",
      "Epoch: 910, Loss: 27.4462, Val: 0.5351, Test: 0.4991\n",
      "Epoch: 920, Loss: 18.6311, Val: 0.5351, Test: 0.4991\n",
      "Epoch: 930, Loss: 46.2128, Val: 0.5351, Test: 0.4991\n",
      "Epoch: 940, Loss: 37.0620, Val: 0.5351, Test: 0.4991\n",
      "Epoch: 950, Loss: 22.0373, Val: 0.5351, Test: 0.4991\n",
      "Epoch: 960, Loss: 17.7365, Val: 0.5351, Test: 0.4991\n",
      "Epoch: 970, Loss: 23.5374, Val: 0.5351, Test: 0.4991\n",
      "Epoch: 980, Loss: 30.5216, Val: 0.5351, Test: 0.4991\n",
      "Epoch: 990, Loss: 24.1037, Val: 0.5375, Test: 0.4866\n",
      "Epoch: 1000, Loss: 25.8058, Val: 0.5375, Test: 0.4866\n",
      "Epoch: 1010, Loss: 11.8180, Val: 0.5375, Test: 0.4866\n",
      "Epoch: 1020, Loss: 15.4006, Val: 0.5375, Test: 0.4866\n",
      "Epoch: 1030, Loss: 22.1727, Val: 0.5375, Test: 0.4866\n",
      "Epoch: 1040, Loss: 17.4859, Val: 0.5375, Test: 0.4866\n",
      "Epoch: 1050, Loss: 19.8556, Val: 0.5375, Test: 0.4866\n",
      "Epoch: 1060, Loss: 29.2329, Val: 0.5375, Test: 0.4866\n",
      "Epoch: 1070, Loss: 18.4915, Val: 0.5375, Test: 0.4866\n",
      "Epoch: 1080, Loss: 25.6951, Val: 0.5375, Test: 0.4866\n",
      "Epoch: 1090, Loss: 22.6070, Val: 0.5375, Test: 0.4866\n",
      "Epoch: 1100, Loss: 17.6298, Val: 0.5375, Test: 0.4866\n",
      "Epoch: 1110, Loss: 25.5410, Val: 0.5375, Test: 0.4866\n",
      "Epoch: 1120, Loss: 16.0908, Val: 0.5375, Test: 0.4866\n",
      "Epoch: 1130, Loss: 15.2969, Val: 0.5375, Test: 0.4866\n",
      "Epoch: 1140, Loss: 15.0280, Val: 0.5375, Test: 0.4866\n",
      "Epoch: 1150, Loss: 15.5447, Val: 0.5375, Test: 0.4866\n",
      "Epoch: 1160, Loss: 19.1447, Val: 0.5375, Test: 0.4866\n",
      "Epoch: 1170, Loss: 12.0192, Val: 0.5375, Test: 0.4866\n",
      "Epoch: 1180, Loss: 19.4947, Val: 0.5375, Test: 0.4866\n",
      "Epoch: 1190, Loss: 26.7026, Val: 0.5375, Test: 0.4866\n",
      "Epoch: 1200, Loss: 13.6218, Val: 0.5375, Test: 0.4866\n",
      "Epoch: 1210, Loss: 23.5582, Val: 0.5375, Test: 0.4866\n",
      "Epoch: 1220, Loss: 14.1696, Val: 0.5375, Test: 0.4866\n",
      "Epoch: 1230, Loss: 18.4523, Val: 0.5375, Test: 0.4866\n",
      "Epoch: 1240, Loss: 15.9847, Val: 0.5375, Test: 0.4866\n",
      "Epoch: 1250, Loss: 18.2462, Val: 0.5375, Test: 0.4866\n",
      "Epoch: 1260, Loss: 12.8431, Val: 0.5375, Test: 0.4866\n",
      "Epoch: 1270, Loss: 7.7333, Val: 0.5375, Test: 0.4866\n",
      "Epoch: 1280, Loss: 28.7372, Val: 0.5375, Test: 0.4866\n",
      "Epoch: 1290, Loss: 12.2882, Val: 0.5375, Test: 0.4866\n",
      "Epoch: 1300, Loss: 15.8501, Val: 0.5375, Test: 0.4866\n",
      "Epoch: 1310, Loss: 13.4439, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 1320, Loss: 9.7567, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 1330, Loss: 23.3475, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 1340, Loss: 25.7176, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 1350, Loss: 12.9519, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 1360, Loss: 11.5484, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 1370, Loss: 9.3348, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 1380, Loss: 23.2388, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 1390, Loss: 14.6180, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 1400, Loss: 18.7959, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 1410, Loss: 12.3796, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 1420, Loss: 11.2394, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 1430, Loss: 42.1252, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 1440, Loss: 22.5183, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 1450, Loss: 16.8365, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 1460, Loss: 18.9446, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 1470, Loss: 9.9793, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 1480, Loss: 14.6724, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 1490, Loss: 9.5792, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 1500, Loss: 12.3924, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 1510, Loss: 13.1829, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 1520, Loss: 10.0400, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 1530, Loss: 11.0373, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 1540, Loss: 17.6983, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 1550, Loss: 14.8194, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 1560, Loss: 11.3042, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 1570, Loss: 9.5682, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 1580, Loss: 17.3345, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 1590, Loss: 9.5524, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 1600, Loss: 12.6321, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 1610, Loss: 25.9599, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 1620, Loss: 15.8403, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 1630, Loss: 18.4646, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 1640, Loss: 11.8205, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 1650, Loss: 9.5207, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 1660, Loss: 13.2859, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 1670, Loss: 17.9705, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 1680, Loss: 13.7681, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 1690, Loss: 8.2670, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 1700, Loss: 7.3956, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 1710, Loss: 10.8134, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 1720, Loss: 15.8134, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 1730, Loss: 11.6992, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 1740, Loss: 13.3509, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 1750, Loss: 18.0493, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 1760, Loss: 10.8944, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 1770, Loss: 17.6070, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 1780, Loss: 7.5345, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 1790, Loss: 11.2497, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 1800, Loss: 11.1115, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 1810, Loss: 7.4278, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 1820, Loss: 11.1206, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 1830, Loss: 9.4376, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 1840, Loss: 10.6163, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 1850, Loss: 8.4267, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 1860, Loss: 14.6316, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 1870, Loss: 7.9454, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 1880, Loss: 13.8313, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 1890, Loss: 12.2093, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 1900, Loss: 7.7843, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 1910, Loss: 11.7351, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 1920, Loss: 11.6417, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 1930, Loss: 9.6446, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 1940, Loss: 5.1773, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 1950, Loss: 6.8978, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 1960, Loss: 6.7049, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 1970, Loss: 14.2269, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 1980, Loss: 5.5551, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 1990, Loss: 12.1678, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2000, Loss: 6.0574, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2010, Loss: 8.6227, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2020, Loss: 7.7989, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2030, Loss: 13.1500, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2040, Loss: 10.9696, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2050, Loss: 6.0106, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2060, Loss: 6.4240, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2070, Loss: 5.3405, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2080, Loss: 6.2843, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2090, Loss: 6.5957, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2100, Loss: 5.7977, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2110, Loss: 5.6148, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2120, Loss: 9.5515, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2130, Loss: 5.9044, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2140, Loss: 5.7952, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2150, Loss: 7.1027, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2160, Loss: 8.6020, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2170, Loss: 7.1753, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2180, Loss: 7.5244, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2190, Loss: 5.7713, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2200, Loss: 11.1610, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2210, Loss: 10.2588, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2220, Loss: 7.0602, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2230, Loss: 5.0716, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2240, Loss: 7.7206, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2250, Loss: 4.8203, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2260, Loss: 5.5049, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2270, Loss: 9.0559, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2280, Loss: 8.8568, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2290, Loss: 6.4444, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2300, Loss: 5.2552, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2310, Loss: 10.1708, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2320, Loss: 7.0980, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2330, Loss: 5.1664, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2340, Loss: 9.0998, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2350, Loss: 11.2160, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2360, Loss: 5.7074, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2370, Loss: 8.5280, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2380, Loss: 9.8511, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2390, Loss: 4.6184, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2400, Loss: 5.5680, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2410, Loss: 12.9900, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2420, Loss: 4.3451, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2430, Loss: 19.3141, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2440, Loss: 6.1176, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2450, Loss: 7.9373, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2460, Loss: 6.6776, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2470, Loss: 5.9152, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2480, Loss: 5.5163, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2490, Loss: 14.0207, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2500, Loss: 5.5854, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2510, Loss: 8.5765, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2520, Loss: 5.0398, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2530, Loss: 5.3121, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2540, Loss: 5.9800, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2550, Loss: 7.7589, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2560, Loss: 8.0128, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2570, Loss: 7.4864, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2580, Loss: 4.0239, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2590, Loss: 6.0821, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2600, Loss: 5.7366, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2610, Loss: 9.3775, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2620, Loss: 4.2301, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2630, Loss: 3.9643, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2640, Loss: 4.3348, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2650, Loss: 4.4798, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2660, Loss: 6.4482, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2670, Loss: 3.2000, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2680, Loss: 4.9289, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2690, Loss: 4.9530, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2700, Loss: 4.4864, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2710, Loss: 7.5292, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2720, Loss: 6.2292, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2730, Loss: 7.0521, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2740, Loss: 2.8910, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2750, Loss: 3.2353, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2760, Loss: 4.1338, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2770, Loss: 5.9002, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2780, Loss: 5.3088, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2790, Loss: 7.4074, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2800, Loss: 4.0609, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2810, Loss: 3.7092, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2820, Loss: 3.2126, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2830, Loss: 4.3768, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2840, Loss: 4.1932, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2850, Loss: 4.7627, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2860, Loss: 6.3009, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2870, Loss: 4.8486, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2880, Loss: 3.7385, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2890, Loss: 3.9776, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2900, Loss: 4.7085, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2910, Loss: 3.2920, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2920, Loss: 3.2551, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2930, Loss: 2.9999, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2940, Loss: 4.0436, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2950, Loss: 4.4135, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2960, Loss: 3.1717, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2970, Loss: 4.1610, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2980, Loss: 4.8691, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 2990, Loss: 6.6423, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3000, Loss: 2.2922, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3010, Loss: 5.4737, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3020, Loss: 3.7556, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3030, Loss: 8.6815, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3040, Loss: 6.2142, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3050, Loss: 4.3115, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3060, Loss: 3.4657, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3070, Loss: 3.8170, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3080, Loss: 2.7289, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3090, Loss: 3.7722, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3100, Loss: 6.2599, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3110, Loss: 4.2571, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3120, Loss: 2.4902, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3130, Loss: 4.7417, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3140, Loss: 4.8076, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3150, Loss: 3.3280, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3160, Loss: 2.7143, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3170, Loss: 3.1301, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3180, Loss: 3.6234, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3190, Loss: 3.2449, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3200, Loss: 3.2475, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3210, Loss: 4.0331, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3220, Loss: 2.5871, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3230, Loss: 4.2811, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3240, Loss: 2.9797, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3250, Loss: 3.0087, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3260, Loss: 3.7394, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3270, Loss: 4.2878, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3280, Loss: 4.5683, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3290, Loss: 2.4561, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3300, Loss: 3.1175, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3310, Loss: 4.2349, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3320, Loss: 4.9438, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3330, Loss: 6.7061, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3340, Loss: 3.1408, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3350, Loss: 2.6523, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3360, Loss: 3.2832, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3370, Loss: 4.5092, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3380, Loss: 2.0097, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3390, Loss: 4.2646, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3400, Loss: 2.5892, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3410, Loss: 2.5473, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3420, Loss: 2.6233, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3430, Loss: 3.0291, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3440, Loss: 2.6047, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3450, Loss: 2.3343, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3460, Loss: 2.4155, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3470, Loss: 3.3984, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3480, Loss: 4.2790, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3490, Loss: 3.6733, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3500, Loss: 3.1056, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3510, Loss: 4.0381, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3520, Loss: 2.2912, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3530, Loss: 2.6030, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3540, Loss: 2.6561, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3550, Loss: 2.4826, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3560, Loss: 3.6429, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3570, Loss: 1.9835, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3580, Loss: 3.2197, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3590, Loss: 3.4752, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3600, Loss: 3.0269, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3610, Loss: 2.9101, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3620, Loss: 2.5293, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3630, Loss: 4.2385, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3640, Loss: 1.8375, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3650, Loss: 2.7335, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3660, Loss: 4.0725, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3670, Loss: 2.7754, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3680, Loss: 2.8398, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3690, Loss: 2.4464, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3700, Loss: 2.5338, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3710, Loss: 2.9835, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3720, Loss: 2.1241, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3730, Loss: 2.1030, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3740, Loss: 1.8609, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3750, Loss: 2.4086, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3760, Loss: 2.0170, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3770, Loss: 2.2492, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3780, Loss: 2.8087, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3790, Loss: 1.7986, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3800, Loss: 3.8051, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3810, Loss: 2.5376, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3820, Loss: 2.7597, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3830, Loss: 2.3826, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3840, Loss: 3.3010, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3850, Loss: 2.1366, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3860, Loss: 4.7356, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3870, Loss: 2.5536, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3880, Loss: 3.0416, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3890, Loss: 2.2143, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3900, Loss: 5.1040, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3910, Loss: 2.8091, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3920, Loss: 2.1567, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3930, Loss: 2.1090, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3940, Loss: 2.3949, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3950, Loss: 2.4929, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3960, Loss: 2.0889, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3970, Loss: 2.3256, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3980, Loss: 1.5975, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 3990, Loss: 1.7053, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4000, Loss: 2.7608, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4010, Loss: 3.1148, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4020, Loss: 3.3801, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4030, Loss: 2.6873, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4040, Loss: 1.6195, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4050, Loss: 3.5126, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4060, Loss: 1.9069, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4070, Loss: 2.9440, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4080, Loss: 1.6095, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4090, Loss: 2.5297, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4100, Loss: 1.4927, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4110, Loss: 1.4974, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4120, Loss: 1.9360, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4130, Loss: 1.8811, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4140, Loss: 1.9724, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4150, Loss: 1.9172, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4160, Loss: 2.1799, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4170, Loss: 1.8165, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4180, Loss: 1.7393, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4190, Loss: 2.0534, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4200, Loss: 1.9825, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4210, Loss: 1.9930, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4220, Loss: 1.3880, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4230, Loss: 1.7031, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4240, Loss: 2.9074, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4250, Loss: 2.3486, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4260, Loss: 2.1258, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4270, Loss: 2.5822, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4280, Loss: 2.1417, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4290, Loss: 3.8381, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4300, Loss: 1.7614, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4310, Loss: 2.1523, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4320, Loss: 3.8920, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4330, Loss: 1.4786, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4340, Loss: 3.1979, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4350, Loss: 1.9688, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4360, Loss: 2.9174, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4370, Loss: 3.1395, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4380, Loss: 2.0108, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4390, Loss: 2.0460, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4400, Loss: 2.5936, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4410, Loss: 1.5108, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4420, Loss: 1.5495, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4430, Loss: 2.3421, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4440, Loss: 3.2207, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4450, Loss: 2.1808, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4460, Loss: 2.1614, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4470, Loss: 2.0324, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4480, Loss: 2.6641, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4490, Loss: 2.3539, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4500, Loss: 2.5105, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4510, Loss: 1.8361, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4520, Loss: 1.5276, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4530, Loss: 1.7874, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4540, Loss: 1.9183, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4550, Loss: 2.6218, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4560, Loss: 1.4150, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4570, Loss: 1.2535, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4580, Loss: 1.9308, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4590, Loss: 1.9038, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4600, Loss: 1.6392, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4610, Loss: 1.7266, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4620, Loss: 1.5718, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4630, Loss: 1.6749, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4640, Loss: 1.3584, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4650, Loss: 1.4489, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4660, Loss: 2.5854, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4670, Loss: 1.8286, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4680, Loss: 1.2197, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4690, Loss: 1.5643, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4700, Loss: 1.7309, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4710, Loss: 2.0390, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4720, Loss: 1.5069, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4730, Loss: 1.1502, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4740, Loss: 1.3382, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4750, Loss: 1.2333, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4760, Loss: 1.5104, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4770, Loss: 1.7151, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4780, Loss: 1.2032, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4790, Loss: 1.3398, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4800, Loss: 1.1962, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4810, Loss: 2.2148, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4820, Loss: 1.8382, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4830, Loss: 1.5498, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4840, Loss: 1.9190, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4850, Loss: 1.4977, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4860, Loss: 1.3774, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4870, Loss: 1.5621, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4880, Loss: 1.4754, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4890, Loss: 1.0558, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4900, Loss: 1.4792, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4910, Loss: 1.5891, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4920, Loss: 1.8676, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4930, Loss: 1.4712, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4940, Loss: 2.0325, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4950, Loss: 1.2017, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4960, Loss: 1.3372, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4970, Loss: 1.3177, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4980, Loss: 1.1177, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 4990, Loss: 1.2008, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5000, Loss: 1.3353, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5010, Loss: 1.6428, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5020, Loss: 1.1044, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5030, Loss: 1.4613, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5040, Loss: 1.1105, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5050, Loss: 1.5937, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5060, Loss: 1.9459, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5070, Loss: 1.4159, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5080, Loss: 1.5052, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5090, Loss: 1.1895, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5100, Loss: 1.4680, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5110, Loss: 1.4745, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5120, Loss: 1.3919, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5130, Loss: 1.1806, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5140, Loss: 1.3028, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5150, Loss: 1.7055, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5160, Loss: 1.2102, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5170, Loss: 1.5320, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5180, Loss: 1.2462, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5190, Loss: 1.2718, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5200, Loss: 1.1023, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5210, Loss: 1.4596, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5220, Loss: 1.0970, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5230, Loss: 1.1575, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5240, Loss: 1.0803, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5250, Loss: 3.1366, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5260, Loss: 2.1088, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5270, Loss: 1.3904, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5280, Loss: 2.0525, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5290, Loss: 1.0507, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5300, Loss: 1.0689, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5310, Loss: 1.0082, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5320, Loss: 0.9731, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5330, Loss: 1.6592, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5340, Loss: 1.3321, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5350, Loss: 0.8720, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5360, Loss: 0.9844, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5370, Loss: 1.4632, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5380, Loss: 1.3835, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5390, Loss: 1.1201, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5400, Loss: 1.1692, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5410, Loss: 1.1356, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5420, Loss: 1.1726, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5430, Loss: 1.1890, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5440, Loss: 1.1403, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5450, Loss: 1.0697, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5460, Loss: 1.1639, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5470, Loss: 1.3577, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5480, Loss: 1.1046, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5490, Loss: 0.9444, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5500, Loss: 1.1074, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5510, Loss: 1.2749, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5520, Loss: 1.0205, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5530, Loss: 1.1006, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5540, Loss: 2.1309, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5550, Loss: 1.2574, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5560, Loss: 1.1240, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5570, Loss: 1.1180, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5580, Loss: 1.1272, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5590, Loss: 0.8771, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5600, Loss: 1.1056, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5610, Loss: 1.0091, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5620, Loss: 1.4166, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5630, Loss: 1.5388, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5640, Loss: 1.2754, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5650, Loss: 0.8484, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5660, Loss: 1.3086, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5670, Loss: 1.0059, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5680, Loss: 1.6358, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5690, Loss: 1.3584, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5700, Loss: 0.9862, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5710, Loss: 1.3002, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5720, Loss: 1.8242, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5730, Loss: 1.0999, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5740, Loss: 1.0899, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5750, Loss: 1.1501, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5760, Loss: 1.1673, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5770, Loss: 1.2743, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5780, Loss: 1.0008, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5790, Loss: 1.0655, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5800, Loss: 1.5844, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5810, Loss: 1.1621, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5820, Loss: 1.3207, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5830, Loss: 0.9057, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5840, Loss: 1.0917, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5850, Loss: 0.9226, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5860, Loss: 0.8502, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5870, Loss: 1.1192, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5880, Loss: 3.3524, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5890, Loss: 1.1786, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5900, Loss: 3.0800, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5910, Loss: 1.4090, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5920, Loss: 1.3131, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5930, Loss: 1.2367, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5940, Loss: 1.1303, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5950, Loss: 1.2139, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5960, Loss: 0.9506, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5970, Loss: 1.0531, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5980, Loss: 1.2003, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 5990, Loss: 1.4264, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6000, Loss: 0.9015, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6010, Loss: 1.0107, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6020, Loss: 1.5512, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6030, Loss: 1.0433, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6040, Loss: 0.9676, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6050, Loss: 0.9097, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6060, Loss: 0.9807, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6070, Loss: 1.0052, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6080, Loss: 1.1355, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6090, Loss: 1.0065, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6100, Loss: 0.9162, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6110, Loss: 1.0918, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6120, Loss: 1.0214, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6130, Loss: 1.0299, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6140, Loss: 1.5860, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6150, Loss: 0.9960, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6160, Loss: 1.0592, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6170, Loss: 1.0919, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6180, Loss: 1.0612, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6190, Loss: 1.2479, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6200, Loss: 1.1373, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6210, Loss: 1.3738, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6220, Loss: 0.9310, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6230, Loss: 1.2519, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6240, Loss: 0.8776, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6250, Loss: 0.9338, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6260, Loss: 0.9124, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6270, Loss: 0.9065, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6280, Loss: 0.8470, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6290, Loss: 0.8911, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6300, Loss: 1.2920, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6310, Loss: 0.9547, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6320, Loss: 0.9529, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6330, Loss: 0.8655, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6340, Loss: 0.8743, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6350, Loss: 0.8751, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6360, Loss: 1.2337, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6370, Loss: 0.8772, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6380, Loss: 0.8984, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6390, Loss: 0.8888, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6400, Loss: 1.0266, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6410, Loss: 1.2114, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6420, Loss: 0.9010, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6430, Loss: 0.9910, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6440, Loss: 0.9331, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6450, Loss: 0.9010, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6460, Loss: 0.8390, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6470, Loss: 0.7941, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6480, Loss: 0.9654, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6490, Loss: 0.8965, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6500, Loss: 0.9087, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6510, Loss: 0.8922, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6520, Loss: 0.8513, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6530, Loss: 0.8694, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6540, Loss: 0.9659, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6550, Loss: 0.9939, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6560, Loss: 0.8400, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6570, Loss: 0.8846, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6580, Loss: 0.9916, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6590, Loss: 0.7982, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6600, Loss: 0.9140, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6610, Loss: 0.9035, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6620, Loss: 0.8435, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6630, Loss: 0.9582, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6640, Loss: 0.8126, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6650, Loss: 0.7691, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6660, Loss: 0.8057, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6670, Loss: 0.8893, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6680, Loss: 0.7838, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6690, Loss: 0.8176, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6700, Loss: 0.7835, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6710, Loss: 0.8522, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6720, Loss: 0.8218, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6730, Loss: 0.8218, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6740, Loss: 0.8309, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6750, Loss: 0.8069, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6760, Loss: 0.8538, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6770, Loss: 0.8508, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6780, Loss: 0.8036, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6790, Loss: 0.7781, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6800, Loss: 0.8016, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6810, Loss: 0.8384, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6820, Loss: 0.8566, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6830, Loss: 0.8899, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6840, Loss: 1.0645, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6850, Loss: 1.2938, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6860, Loss: 0.7673, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6870, Loss: 0.7399, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6880, Loss: 0.8099, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6890, Loss: 0.8254, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6900, Loss: 0.8658, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6910, Loss: 0.7651, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6920, Loss: 0.7750, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6930, Loss: 0.7819, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6940, Loss: 0.8758, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6950, Loss: 0.8310, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6960, Loss: 0.8250, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6970, Loss: 0.7795, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6980, Loss: 0.7869, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 6990, Loss: 0.8308, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7000, Loss: 0.7303, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7010, Loss: 0.8654, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7020, Loss: 0.7756, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7030, Loss: 1.0188, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7040, Loss: 0.7582, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7050, Loss: 0.7724, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7060, Loss: 0.9389, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7070, Loss: 0.8039, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7080, Loss: 0.9032, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7090, Loss: 0.9964, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7100, Loss: 1.1337, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7110, Loss: 0.7877, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7120, Loss: 0.7603, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7130, Loss: 0.7578, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7140, Loss: 0.9295, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7150, Loss: 0.8149, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7160, Loss: 0.8047, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7170, Loss: 0.7537, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7180, Loss: 0.7766, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7190, Loss: 0.7724, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7200, Loss: 0.7774, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7210, Loss: 0.8405, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7220, Loss: 0.8620, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7230, Loss: 0.8999, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7240, Loss: 0.8030, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7250, Loss: 0.7469, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7260, Loss: 0.9100, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7270, Loss: 0.7502, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7280, Loss: 0.7453, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7290, Loss: 0.8249, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7300, Loss: 0.7371, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7310, Loss: 0.7823, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7320, Loss: 0.8043, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7330, Loss: 0.7444, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7340, Loss: 0.7247, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7350, Loss: 0.8532, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7360, Loss: 0.7466, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7370, Loss: 0.7598, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7380, Loss: 0.7467, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7390, Loss: 0.7383, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7400, Loss: 0.7655, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7410, Loss: 0.7343, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7420, Loss: 0.7916, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7430, Loss: 0.7584, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7440, Loss: 0.8975, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7450, Loss: 0.7503, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7460, Loss: 0.8231, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7470, Loss: 0.7657, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7480, Loss: 0.7526, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7490, Loss: 0.7371, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7500, Loss: 0.7624, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7510, Loss: 0.7308, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7520, Loss: 0.8952, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7530, Loss: 0.7524, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7540, Loss: 0.8082, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7550, Loss: 0.7487, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7560, Loss: 0.7728, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7570, Loss: 0.7727, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7580, Loss: 0.7710, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7590, Loss: 0.7695, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7600, Loss: 0.7247, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7610, Loss: 0.7368, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7620, Loss: 0.7613, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7630, Loss: 0.7459, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7640, Loss: 0.7843, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7650, Loss: 0.7127, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7660, Loss: 0.7727, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7670, Loss: 0.8598, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7680, Loss: 0.8195, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7690, Loss: 0.7614, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7700, Loss: 0.7762, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7710, Loss: 0.7278, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7720, Loss: 0.7584, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7730, Loss: 0.7429, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7740, Loss: 0.9716, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7750, Loss: 0.7027, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7760, Loss: 1.0279, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7770, Loss: 0.7431, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7780, Loss: 0.7545, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7790, Loss: 0.7323, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7800, Loss: 0.7218, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7810, Loss: 0.7626, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7820, Loss: 0.7497, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7830, Loss: 0.7712, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7840, Loss: 0.7534, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7850, Loss: 0.7364, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7860, Loss: 0.7544, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7870, Loss: 0.7964, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7880, Loss: 0.7639, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7890, Loss: 0.7464, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7900, Loss: 0.7319, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7910, Loss: 0.7771, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7920, Loss: 0.7576, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7930, Loss: 0.7631, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7940, Loss: 0.7684, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7950, Loss: 0.7484, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7960, Loss: 0.7347, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7970, Loss: 0.7231, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7980, Loss: 0.7762, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 7990, Loss: 0.7253, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 8000, Loss: 0.7249, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 8010, Loss: 0.7280, Val: 0.5413, Test: 0.5094\n",
      "Epoch: 8020, Loss: 0.7536, Val: 0.5413, Test: 0.5094\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m best_val_perf \u001b[38;5;241m=\u001b[39m test_perf \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m100000\u001b[39m):\n\u001b[1;32m----> 3\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     val_perf, tmp_test_perf \u001b[38;5;241m=\u001b[39m test()\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m val_perf \u001b[38;5;241m>\u001b[39m best_val_perf:\n",
      "Cell \u001b[1;32mIn[52], line 14\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m():\n\u001b[0;32m     12\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m---> 14\u001b[0m     neg_edge_index \u001b[38;5;241m=\u001b[39m \u001b[43mnegative_sampling\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43medge_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_pos_edge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#positive edges\u001b[39;49;00m\n\u001b[0;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_nodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_nodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# number of nodes\u001b[39;49;00m\n\u001b[0;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_neg_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_pos_edge_index\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# number of neg_sample equal to number of pos_edges\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     21\u001b[0m     z \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode() \u001b[38;5;66;03m#encode\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Lucas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch_geometric\\utils\\_negative_sampling.py:103\u001b[0m, in \u001b[0;36mnegative_sampling\u001b[1;34m(edge_index, num_nodes, num_neg_samples, method, force_undirected)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m):  \u001b[38;5;66;03m# Number of tries to sample negative indices.\u001b[39;00m\n\u001b[0;32m    102\u001b[0m     rnd \u001b[38;5;241m=\u001b[39m sample(population, sample_size, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 103\u001b[0m     mask \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrnd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m neg_idx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    105\u001b[0m         mask \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39misin(rnd, neg_idx\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36misin\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Lucas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\lib\\arraysetops.py:739\u001b[0m, in \u001b[0;36misin\u001b[1;34m(element, test_elements, assume_unique, invert)\u001b[0m\n\u001b[0;32m    646\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    647\u001b[0m \u001b[38;5;124;03mCalculates ``element in test_elements``, broadcasting over `element` only.\u001b[39;00m\n\u001b[0;32m    648\u001b[0m \u001b[38;5;124;03mReturns a boolean array of the same shape as `element` that is True\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    736\u001b[0m \u001b[38;5;124;03m       [ True, False]])\u001b[39;00m\n\u001b[0;32m    737\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    738\u001b[0m element \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(element)\n\u001b[1;32m--> 739\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43min1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43melement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_elements\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43massume_unique\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43massume_unique\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    740\u001b[0m \u001b[43m            \u001b[49m\u001b[43minvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minvert\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreshape(element\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36min1d\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Lucas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\lib\\arraysetops.py:617\u001b[0m, in \u001b[0;36min1d\u001b[1;34m(ar1, ar2, assume_unique, invert)\u001b[0m\n\u001b[0;32m    615\u001b[0m \u001b[38;5;66;03m# Otherwise use sorting\u001b[39;00m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m assume_unique:\n\u001b[1;32m--> 617\u001b[0m     ar1, rev_idx \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43mar1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_inverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    618\u001b[0m     ar2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(ar2)\n\u001b[0;32m    620\u001b[0m ar \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((ar1, ar2))\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36munique\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Lucas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\lib\\arraysetops.py:274\u001b[0m, in \u001b[0;36munique\u001b[1;34m(ar, return_index, return_inverse, return_counts, axis, equal_nan)\u001b[0m\n\u001b[0;32m    272\u001b[0m ar \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masanyarray(ar)\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 274\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43m_unique1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_inverse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_counts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mequal_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mequal_nan\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unpack_tuple(ret)\n\u001b[0;32m    278\u001b[0m \u001b[38;5;66;03m# axis was specified and not None\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Lucas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\lib\\arraysetops.py:362\u001b[0m, in \u001b[0;36m_unique1d\u001b[1;34m(ar, return_index, return_inverse, return_counts, equal_nan)\u001b[0m\n\u001b[0;32m    360\u001b[0m     inv_idx[perm] \u001b[38;5;241m=\u001b[39m imask\n\u001b[0;32m    361\u001b[0m     ret \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (inv_idx,)\n\u001b[1;32m--> 362\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_counts:\n\u001b[0;32m    363\u001b[0m     idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(np\u001b[38;5;241m.\u001b[39mnonzero(mask) \u001b[38;5;241m+\u001b[39m ([mask\u001b[38;5;241m.\u001b[39msize],))\n\u001b[0;32m    364\u001b[0m     ret \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (np\u001b[38;5;241m.\u001b[39mdiff(idx),)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "best_val_perf = test_perf = 0\n",
    "for epoch in range(1, 100000):\n",
    "    train_loss = train()\n",
    "    val_perf, tmp_test_perf = test()\n",
    "    if val_perf > best_val_perf:\n",
    "        best_val_perf = val_perf\n",
    "        test_perf = tmp_test_perf\n",
    "    log = 'Epoch: {:03d}, Loss: {:.4f}, Val: {:.4f}, Test: {:.4f}'\n",
    "    if epoch % 10 == 0:\n",
    "        print(log.format(epoch, train_loss, best_val_perf, test_perf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 200)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "latent_embeddings =  torch.tensor(1)\n",
    "\n",
    "z = model.encode()\n",
    "\n",
    "with torch.no_grad():\n",
    "    if len(latent_embeddings.shape) == 0:\n",
    "            latent_embeddings = z\n",
    "    else:\n",
    "        latent_embeddings = torch.cat((latent_embeddings,z))\n",
    "\n",
    "latent_embeddings = latent_embeddings.cpu().detach().numpy()\n",
    "print(latent_embeddings.shape)\n",
    "tsne = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "reduced_embeddings = tsne.fit_transform(latent_embeddings)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], s=10)\n",
    "plt.title(\"Latent Space Visualization (t-SNE)\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
