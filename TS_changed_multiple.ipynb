{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lucas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56\n",
      "This choice will generate autonomous dynamics\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = 'cpu'\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from models.training import create_dataloader\n",
    "import scipy.io as io\n",
    "\n",
    "\n",
    "# Juptyer magic: For export. Makes the plots size right for the screen \n",
    "%matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "%config InlineBackend.figure_formats = ['svg'] \n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "seed = np.random.randint(1,200)\n",
    "seed = 56\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "print(seed)\n",
    "g = torch.Generator()\n",
    "g.manual_seed(seed)\n",
    "\n",
    "# design choices \n",
    "chosen_problem = 'restrictedTS_alt'\n",
    "data_noise = 0.\n",
    "n_different_weights = 1\n",
    "if n_different_weights == 1:\n",
    "    print('This choice will generate autonomous dynamics')\n",
    "else:\n",
    "    print('This choice generates non-autonomous dynamics, letting the weights depend on time')\n",
    "\n",
    "possible_problem = {'moons':'moons', 'ToggleSwitch':'TS', 'repressilator':'repr', 'restricted_TS': 'restrictedTS', 'restrictedTS_alt':'restrictedTS_alt'} \n",
    "# this choices determine the data set that we build and subsequent choices on the construction of the neural ODE \n",
    "# - in particular, it determines the dimensions \n",
    "problem = possible_problem[chosen_problem]\n",
    "\n",
    "plotlim = [0, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check whether a changed problem affects the outcome of our methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restrictedTS_alt\n",
      "No change  applied to TS or repr data\n",
      "0\n",
      "[[4.765574  2.803489 ]\n",
      " [1.7676971 4.7275352]]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (2x2 and 3x3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 69\u001b[0m\n\u001b[0;32m     65\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmanual_seed(seed)\n\u001b[0;32m     66\u001b[0m trainer_anode \u001b[38;5;241m=\u001b[39m doublebackTrainer(anode, optimizer_anode, device, cross_entropy\u001b[38;5;241m=\u001b[39mcross_entropy, turnpike \u001b[38;5;241m=\u001b[39m turnpike,\n\u001b[0;32m     67\u001b[0m                         bound\u001b[38;5;241m=\u001b[39mbound, fixed_projector\u001b[38;5;241m=\u001b[39mfp, verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, eps_comp \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.2\u001b[39m)\n\u001b[1;32m---> 69\u001b[0m \u001b[43mtrainer_anode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m W1 \u001b[38;5;241m=\u001b[39m anode\u001b[38;5;241m.\u001b[39mflow\u001b[38;5;241m.\u001b[39mdynamics\u001b[38;5;241m.\u001b[39mfc1_time[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mweight\n\u001b[0;32m     72\u001b[0m W1 \u001b[38;5;241m=\u001b[39m W1\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[1;32mc:\\Data\\Code\\BioML\\models\\training.py:80\u001b[0m, in \u001b[0;36mdoublebackTrainer.train\u001b[1;34m(self, data_loader, num_epochs)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m, data_loader, num_epochs):\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m---> 80\u001b[0m         avg_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     81\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose:\n\u001b[0;32m     82\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{:.3f}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, avg_loss))\n",
      "File \u001b[1;32mc:\\Data\\Code\\BioML\\models\\training.py:100\u001b[0m, in \u001b[0;36mdoublebackTrainer._train_epoch\u001b[1;34m(self, data_loader, epoch)\u001b[0m\n\u001b[0;32m     97\u001b[0m x_batch \u001b[38;5;241m=\u001b[39m x_batch\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     98\u001b[0m y_batch \u001b[38;5;241m=\u001b[39m y_batch\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m--> 100\u001b[0m y_pred, traj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    101\u001b[0m time_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtime_steps\n\u001b[0;32m    102\u001b[0m T \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mT\n",
      "File \u001b[1;32mc:\\Users\\Lucas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Data\\Code\\BioML\\models\\neural_odes.py:267\u001b[0m, in \u001b[0;36mNeuralODE.forward\u001b[1;34m(self, x, return_features)\u001b[0m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, return_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m--> 267\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    269\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflow\u001b[38;5;241m.\u001b[39mtrajectory(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_steps)\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;66;03m#pred = self.linear_layer(features)\u001b[39;00m\n\u001b[0;32m    271\u001b[0m     \u001b[38;5;66;03m#self.proj_traj = self.linear_layer(self.traj)\u001b[39;00m\n\u001b[0;32m    272\u001b[0m     \u001b[38;5;66;03m#if not self.cross_entropy:\u001b[39;00m\n\u001b[0;32m    273\u001b[0m     \u001b[38;5;66;03m#    pred = self.non_linearity(pred)\u001b[39;00m\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;66;03m#    self.proj_traj = self.non_linearity(self.proj_traj)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Lucas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Data\\Code\\BioML\\models\\neural_odes.py:208\u001b[0m, in \u001b[0;36mSemiflow.forward\u001b[1;34m(self, x, eval_times)\u001b[0m\n\u001b[0;32m    202\u001b[0m     out \u001b[38;5;241m=\u001b[39m odeint_adjoint(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdynamics, x_aug, integration_time, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meuler\u001b[39m\u001b[38;5;124m'\u001b[39m, options\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep_size\u001b[39m\u001b[38;5;124m'\u001b[39m: dt})\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;66;03m# out = odeint_adjoint(self.dynamics, x_aug, integration_time, method='dopri5', rtol = 0.1, atol = 0.1)\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;66;03m# ÃŸout = odeint(self.dynamics, x_aug, integration_time, method='euler', options={'step_size': dt})\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43modeint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdynamics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_aug\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mintegration_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdopri5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrtol\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43matol\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    210\u001b[0m     \u001b[38;5;66;03m# i need to put the out into the odeint for the adj_out\u001b[39;00m\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;66;03m# adj_out = odeint(self.adj_dynamics, torch.eye(x.shape[0]), torch.flip(integration_time,[0]), method='euler', options={'step_size': dt}) #this is new for the adjoint\u001b[39;00m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m eval_times \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Lucas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchdiffeq\\_impl\\odeint.py:77\u001b[0m, in \u001b[0;36modeint\u001b[1;34m(func, y0, t, rtol, atol, method, options, event_fn)\u001b[0m\n\u001b[0;32m     74\u001b[0m solver \u001b[38;5;241m=\u001b[39m SOLVERS[method](func\u001b[38;5;241m=\u001b[39mfunc, y0\u001b[38;5;241m=\u001b[39my0, rtol\u001b[38;5;241m=\u001b[39mrtol, atol\u001b[38;5;241m=\u001b[39matol, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 77\u001b[0m     solution \u001b[38;5;241m=\u001b[39m \u001b[43msolver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintegrate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m     event_t, solution \u001b[38;5;241m=\u001b[39m solver\u001b[38;5;241m.\u001b[39mintegrate_until_event(t[\u001b[38;5;241m0\u001b[39m], event_fn)\n",
      "File \u001b[1;32mc:\\Users\\Lucas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchdiffeq\\_impl\\solvers.py:28\u001b[0m, in \u001b[0;36mAdaptiveStepsizeODESolver.integrate\u001b[1;34m(self, t)\u001b[0m\n\u001b[0;32m     26\u001b[0m solution[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my0\n\u001b[0;32m     27\u001b[0m t \u001b[38;5;241m=\u001b[39m t\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_before_integrate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(t)):\n\u001b[0;32m     30\u001b[0m     solution[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_advance(t[i])\n",
      "File \u001b[1;32mc:\\Users\\Lucas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchdiffeq\\_impl\\rk_common.py:161\u001b[0m, in \u001b[0;36mRKAdaptiveStepsizeODESolver._before_integrate\u001b[1;34m(self, t)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_before_integrate\u001b[39m(\u001b[38;5;28mself\u001b[39m, t):\n\u001b[0;32m    160\u001b[0m     t0 \u001b[38;5;241m=\u001b[39m t[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m--> 161\u001b[0m     f0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my0\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfirst_step \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    163\u001b[0m         first_step \u001b[38;5;241m=\u001b[39m _select_initial_step(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc, t[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my0, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morder \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrtol, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39matol,\n\u001b[0;32m    164\u001b[0m                                           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm, f0\u001b[38;5;241m=\u001b[39mf0)\n",
      "File \u001b[1;32mc:\\Users\\Lucas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Lucas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchdiffeq\\_impl\\misc.py:189\u001b[0m, in \u001b[0;36m_PerturbFunc.forward\u001b[1;34m(self, t, y, perturb)\u001b[0m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;66;03m# Do nothing.\u001b[39;00m\n\u001b[0;32m    188\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m--> 189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Lucas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Data\\Code\\BioML\\models\\neural_odes.py:133\u001b[0m, in \u001b[0;36mDynamics.forward\u001b[1;34m(self, t, x, verbose)\u001b[0m\n\u001b[0;32m    131\u001b[0m b1_t \u001b[38;5;241m=\u001b[39m  nn\u001b[38;5;241m.\u001b[39mParameter(torch\u001b[38;5;241m.\u001b[39mTensor([\u001b[38;5;241m2.\u001b[39m, \u001b[38;5;241m2.\u001b[39m]), requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    132\u001b[0m \u001b[38;5;66;03m# nn.Parameter(weights2, requires_grad=False)\u001b[39;00m\n\u001b[1;32m--> 133\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnon_linearity(\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw1_t\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m b1_t)\n\u001b[0;32m    134\u001b[0m w2_t \u001b[38;5;241m=\u001b[39m  nn\u001b[38;5;241m.\u001b[39mParameter(torch\u001b[38;5;241m.\u001b[39mTensor([[\u001b[38;5;241m2.\u001b[39m,\u001b[38;5;241m0\u001b[39m],[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m2.\u001b[39m]]), requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    135\u001b[0m b2_t \u001b[38;5;241m=\u001b[39m  nn\u001b[38;5;241m.\u001b[39mParameter(torch\u001b[38;5;241m.\u001b[39mTensor([\u001b[38;5;241m2.\u001b[39m,\u001b[38;5;241m2.\u001b[39m]), requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (2x2 and 3x3)"
     ]
    }
   ],
   "source": [
    "chosen_problem = 'restrictedTS_alt'\n",
    "problem = possible_problem[chosen_problem]\n",
    "\n",
    "ind = 0\n",
    "m1vec = []\n",
    "m2vec = []\n",
    "lossvec = []\n",
    "var1vec = []\n",
    "var2vec = []\n",
    "for sz in range(2,3):\n",
    "\n",
    "    m1vectemp = []\n",
    "    m2vectemp = []\n",
    "    lossvectemp = []\n",
    "    Xtempvec = []\n",
    "    W1vec = []\n",
    "\n",
    "    for k in range(0,500):\n",
    "        \n",
    "        ind = ind + 1\n",
    "        seed = ind\n",
    "\n",
    "        dataloader, dataloader_viz, X_train = create_dataloader(problem, batch_size = sz, noise = data_noise, \n",
    "                                                    plotlim = plotlim, random_state = seed, label = 'vector')\n",
    "        \n",
    "        print(k)\n",
    "        print(X_train.detach().numpy())\n",
    "\n",
    "        #Import of the model dynamics that describe the neural ODE\n",
    "        #The dynamics are based on the torchdiffeq package, that implements ODE solvers in the pytorch setting\n",
    "        from models.neural_odes import NeuralODE\n",
    "\n",
    "        #T is the end time of the neural ODE evolution, num_steps are the amount of discretization steps for the ODE solver\n",
    "        T, num_steps = 1, n_different_weights\n",
    "        bound = 0.\n",
    "        fp = False\n",
    "        cross_entropy = False\n",
    "        turnpike = False\n",
    "\n",
    "        # choice of model: what nonlinearity is used and if the nonlinearity is applied before (inside) or after (outside) the linear weights\n",
    "        # another choice is bottleneck, but I don't understand it\n",
    "        # non_linearity = 'tanh' # OR 'relu' 'sigmoid' 'leakyrelu' 'tanh_prime'\n",
    "        # architecture = 'inside' 'outside'\n",
    "        non_linearity = 'tanh'\n",
    "        architecture = 'restricted'\n",
    "        architectures = {'inside': -1, 'outside': 0, 'bottleneck': 1, 'restricted': 2}\n",
    "        # number of optimization runs in which the dataset is used for gradient decent\n",
    "        num_epochs = 50\n",
    "        if problem == 'moons' or problem == 'TS' or problem == \"restrictedTS\" or problem == \"restrictedTS_alt\":\n",
    "            hidden_dim, data_dim = 2, 2 \n",
    "        else:\n",
    "            hidden_dim, data_dim = 3, 3 \n",
    "        augment_dim = 0\n",
    "\n",
    "        # resets the seed - allows for coherent runs in the gradient descent as well\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        anode = NeuralODE(device, data_dim, hidden_dim, output_dim=data_dim, augment_dim=augment_dim, non_linearity=non_linearity, \n",
    "                            architecture=architecture, T=T, time_steps=num_steps, fixed_projector=fp, cross_entropy=cross_entropy)\n",
    "        optimizer_anode = torch.optim.Adam(anode.parameters(), lr=1e-1)\n",
    "\n",
    "        from models.training import doublebackTrainer\n",
    "\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        trainer_anode = doublebackTrainer(anode, optimizer_anode, device, cross_entropy=cross_entropy, turnpike = turnpike,\n",
    "                                bound=bound, fixed_projector=fp, verbose = False, eps_comp = 0.2)\n",
    "\n",
    "        trainer_anode.train(dataloader, 200)\n",
    "\n",
    "        W1 = anode.flow.dynamics.fc1_time[0].weight\n",
    "        W1 = W1.detach().numpy()\n",
    "        m1 = abs(W1[0][1]-W1[1][0])\n",
    "        m2 = abs(W1[0][0])+abs(W1[1][1])\n",
    "\n",
    "        lv = trainer_anode.histories[\"loss_history\"]\n",
    "        l = lv[-1]\n",
    "\n",
    "        m1vectemp.append(m1)\n",
    "        m2vectemp.append(m2)\n",
    "        lossvectemp.append(l)\n",
    "        Xtempvec.append(X_train.detach().numpy())\n",
    "        W1vec.append(W1)\n",
    "    \n",
    "    mdic = {\"X\":Xtempvec,\"l\":lossvectemp,\"symm\":m1vectemp,\"offdiag\":m2vectemp,\"W1\":W1vec}\n",
    "    io.savemat(\"alt_restricted_n\"+str(sz)+\".mat\",mdic)\n",
    "    m1vec.append(np.mean(m1vectemp))\n",
    "    m2vec.append(np.mean(m2vectemp))\n",
    "    lossvec.append(np.mean(lossvectemp))\n",
    "    var1vec.append(np.var(m1vectemp))\n",
    "    var2vec.append(np.var(m2vectemp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check whether running multiple fits affects our methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_problem = 'restricted_TS'\n",
    "problem = possible_problem[chosen_problem]\n",
    "\n",
    "ind = 0\n",
    "m1vec = []\n",
    "m2vec = []\n",
    "lossvec = []\n",
    "var1vec = []\n",
    "var2vec = []\n",
    "for sz in range(2,3):\n",
    "\n",
    "    for k in range(0,20):\n",
    "\n",
    "        m1vectemp = []\n",
    "        m2vectemp = []\n",
    "        lossvectemp = []\n",
    "        Xtempvec = []\n",
    "        W1vec = []\n",
    "        \n",
    "        ind = ind + 1\n",
    "        seed = ind\n",
    "\n",
    "        dataloader, dataloader_viz, X_train = create_dataloader(problem, batch_size = sz, noise = data_noise, \n",
    "                                                    plotlim = plotlim, random_state = seed, label = 'vector')\n",
    "        \n",
    "        print(k)\n",
    "        print(X_train.detach().numpy())\n",
    "\n",
    "        #Import of the model dynamics that describe the neural ODE\n",
    "        #The dynamics are based on the torchdiffeq package, that implements ODE solvers in the pytorch setting\n",
    "        from models.neural_odes import NeuralODE\n",
    "\n",
    "        #T is the end time of the neural ODE evolution, num_steps are the amount of discretization steps for the ODE solver\n",
    "        T, num_steps = 1, n_different_weights\n",
    "        bound = 0.\n",
    "        fp = False\n",
    "        cross_entropy = False\n",
    "        turnpike = False\n",
    "\n",
    "        # choice of model: what nonlinearity is used and if the nonlinearity is applied before (inside) or after (outside) the linear weights\n",
    "        # another choice is bottleneck, but I don't understand it\n",
    "        # non_linearity = 'tanh' # OR 'relu' 'sigmoid' 'leakyrelu' 'tanh_prime'\n",
    "        # architecture = 'inside' 'outside'\n",
    "        non_linearity = 'tanh'\n",
    "        architecture = 'restricted'\n",
    "        architectures = {'inside': -1, 'outside': 0, 'bottleneck': 1, 'restricted': 2}\n",
    "        # number of optimization runs in which the dataset is used for gradient decent\n",
    "        num_epochs = 50\n",
    "        if problem == 'moons' or problem == 'TS' or problem == \"restrictedTS\":\n",
    "            hidden_dim, data_dim = 2, 2 \n",
    "        else:\n",
    "            hidden_dim, data_dim = 3, 3 \n",
    "        augment_dim = 0\n",
    "\n",
    "        for nsims in range(0,10):\n",
    "\n",
    "            seed = nsims+1\n",
    "\n",
    "            # resets the seed - allows for coherent runs in the gradient descent as well\n",
    "            torch.manual_seed(seed)\n",
    "            torch.cuda.manual_seed(seed)\n",
    "            anode = NeuralODE(device, data_dim, hidden_dim, output_dim=data_dim, augment_dim=augment_dim, non_linearity=non_linearity, \n",
    "                                architecture=architecture, T=T, time_steps=num_steps, fixed_projector=fp, cross_entropy=cross_entropy)\n",
    "            optimizer_anode = torch.optim.Adam(anode.parameters(), lr=1e-1)\n",
    "\n",
    "            from models.training import doublebackTrainer\n",
    "\n",
    "            torch.manual_seed(seed)\n",
    "            torch.cuda.manual_seed(seed)\n",
    "            trainer_anode = doublebackTrainer(anode, optimizer_anode, device, cross_entropy=cross_entropy, turnpike = turnpike,\n",
    "                                    bound=bound, fixed_projector=fp, verbose = False, eps_comp = 0.2)\n",
    "\n",
    "            trainer_anode.train(dataloader, 200)\n",
    "\n",
    "            W1 = anode.flow.dynamics.fc1_time[0].weight\n",
    "            W1 = W1.detach().numpy()\n",
    "            m1 = abs(W1[0][1]-W1[1][0])\n",
    "            m2 = abs(W1[0][0])+abs(W1[1][1])\n",
    "\n",
    "            lv = trainer_anode.histories[\"loss_history\"]\n",
    "            l = lv[-1]\n",
    "\n",
    "            m1vectemp.append(m1)\n",
    "            m2vectemp.append(m2)\n",
    "            lossvectemp.append(l)\n",
    "            Xtempvec.append(X_train.detach().numpy())\n",
    "            W1vec.append(W1)\n",
    "    \n",
    "        mdic = {\"X\":Xtempvec,\"l\":lossvectemp,\"symm\":m1vectemp,\"offdiag\":m2vectemp,\"W1\":W1vec}\n",
    "        io.savemat(\"per_fit_restricted_n\"+str(k)+\".mat\",mdic)\n",
    "        m1vec.append(np.mean(m1vectemp))\n",
    "        m2vec.append(np.mean(m2vectemp))\n",
    "        lossvec.append(np.mean(lossvectemp))\n",
    "        var1vec.append(np.var(m1vectemp))\n",
    "        var2vec.append(np.var(m2vectemp))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
